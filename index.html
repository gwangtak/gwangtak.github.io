<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Gwangtak Bae</title>

    <meta name="author" content="Gwangtak Bae">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="icon" href="images/gt_logo.png" type="image/png" sizes="32x32">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:1100px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <section class="about-card">
            <div class="about-card-inner">
              <h1 class="about-name" style="font-size:2.7em;">Gwangtak Bae</h1>
              <div class="about-photo">
                <a href="images/gwangtak.jpg"><img src="images/gwangtak.jpg" alt="Gwangtak Bae" class="about-photo-img"></a>
              </div>
              <div class="about-text">
                <p>
                  I'm a Ph.D. candidate in the <a href="https://3d.snu.ac.kr/members/" target="_blank">3D Vision Lab</a> at Seoul National University (SNU), advised by <a href="https://3d.snu.ac.kr/members/" target="_blank">Prof. Young Min Kim</a>.
                  Before joining SNU, I worked as a Research Officer at the Agency for Defense Development (ADD), South Korea's counterpart to U.S. DARPA.
                  I received my B.S. in Electrical Engineering from the Korea Advanced Institute of Science and Technology (KAIST).
                </p>
                <p>
                  My research focuses on <b>robust computer vision for robotics.</b>
                  I am particularly interested in identifying real-world challenges robots face and developing practical solutions,
                  with broad experience in <b>3D reconstruction and localization</b>, from classical geometry-based approaches to neural methods.
                </p>
                <p class="about-links">
                  <a href="mailto:tak3452@gmail.com">Email</a> / <a href="data/CV_GwangtakBae_Feb2026.pdf">CV</a> / <a href="https://scholar.google.com/citations?hl=en&user=eg1Jj8UAAAAJ">Scholar</a> / <a href="https://www.linkedin.com/in/gwangtak-bae-749613140/">LinkedIn</a> / <a href="https://github.com/gwangtak/">Github</a>
                </p>
              </div>
            </div>
          </section>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:16px 16px 0 16px;width:100%;vertical-align:middle">
                <h2>News</h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:16px;width:100%;vertical-align:top" class="news-content-cell">
                <ul class="news-list">
                  <li><span class="news-date">Feb 2026:</span> One paper is accepted to IEEE Transactions on Robotics (T-RO).</li>
                  <li><span class="news-date">Jul 2025:</span> One paper is accepted to ICCV 2025.</li>
                  <li><span class="news-date">Jul 2024:</span> One paper is accepted to ECCV 2024.</li>
                  <li class="news-more"><span class="news-date">Jul 2022:</span> One paper is accepted to ECCV 2022.</li>
                </ul>
                <a href="#" class="news-show-more" id="news-toggle" aria-expanded="false">Show more ▼</a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px 16px 0 16px;width:100%;vertical-align:middle">
                <h2>Research</h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:16px;width:100%;vertical-align:top">
                <div class="research-cards">
                  <article class="research-card">
                    <div class="research-card-thumbnail">
                      <img src="images/roel.png" alt="RoEL thumbnail">
                    </div>
                    <div class="research-card-content">
                      <h3 class="research-title">
                        <a href="https://gwangtak.github.io/roel/">RoEL: Robust Event-based 3D Line Reconstruction</a>
                      </h3>
                      <p class="research-authors">
                        <span class="research-author-highlight">Gwangtak Bae</span>,
                        <a href="https://rpm.snu.ac.kr/">Jaeho Shin</a>,
                        <a href="https://3d.snu.ac.kr/members/">Seunggu Kang</a>,
                        <a href="https://www.junhokim.xyz">Junho Kim</a>,
                        <a href="https://rpm.snu.ac.kr/">Ayoung Kim</a>,
                        <a href="https://3d.snu.ac.kr/members/">Young Min Kim</a>
                      </p>
                        <p class="research-venue" style="font-size:1.0em;"><em>IEEE Transactions on Robotics (T-RO)</em>, 2026</p>
                      <p class="research-links">
                        <a href="https://gwangtak.github.io/roel/">Project Page</a> / <a href="https://arxiv.org/abs/2602.18258">arXiv</a> / <a href="https://github.com/gwangtak/roel">Code</a>
                      </p>
                      <p class="research-abstract">
                        <u>What is the best 3D representation for event cameras?</u><br>
                        We introduce a correspondence-based 3D line reconstruction pipeline for event cameras,
                        from reliable correspondence search to Grassmannian optimization in 3D space.
                      </p>
                    </div>
                  </article>

                  <article class="research-card">
                    <div class="research-card-thumbnail">
                      <img src="images/scene_analogy.png" alt="Neural Contextual Scene Maps thumbnail">
                    </div>
                    <div class="research-card-content">
                      <h3 class="research-title">
                        <a href="#">Learning 3D Scene Analogies with Neural Contextual Scene Maps</a>
                      </h3>
                      <p class="research-authors">
                        <a href="https://www.junhokim.xyz">Junho Kim</a>,
                        <span class="research-author-highlight">Gwangtak Bae</span>,
                        <a href="https://sites.google.com/view/eunsunlee">Eunsun Lee</a>,
                        <a href="https://3d.snu.ac.kr/members/">Young Min Kim</a>
                      </p>
                      <p class="research-venue"><em>International Conference on Computer Vision (ICCV)</em>, 2025</p>
                      <p class="research-links">
                        <a href="https://82magnolia.github.io/3d_scene_analogies/">Project Page</a> / <a href="https://arxiv.org/abs/2503.15897">arXiv</a> / <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Kim_Learning_3D_Scene_Analogies_with_Neural_Contextual_Scene_Maps_ICCV_2025_paper.html">Paper</a> / <a href="https://openaccess.thecvf.com/content/ICCV2025/supplemental/Kim_Learning_3D_Scene_ICCV_2025_supplemental.pdf">Supp</a> / <a href="https://github.com/82magnolia/3d_scene_analogies/">Code</a>
                      </p>
                      <p class="research-abstract">
                        <u>Can we find correspondences between different but similar 3D scenes?</u><br>
                        We propose a new task of finding 3D scene analogies, which are dense maps connecting regions sharing similar scene contexts.
                      </p>
                    </div>
                  </article>

                  <article class="research-card">
                    <div class="research-card-thumbnail">
                      <img src="images/i2slam.gif" alt="I²-SLAM thumbnail">
                    </div>
                    <div class="research-card-content">
                      <h3 class="research-title">
                        <a href="#">I<sup>2</sup>-SLAM: Inverting Imaging Process for Robust Photorealistic Dense SLAM</a>
                      </h3>
                      <p class="research-authors">
                        <span class="research-author-highlight">Gwangtak Bae</span>*,
                        <a href="https://www.changwoon.info/">Changwoon Choi</a>*,
                        <a href="https://heo0224.github.io/">Hyeongjun Heo</a>,
                        <a href="https://sangminkim-99.github.io/">Sang Min Kim</a>,
                        <a href="https://3d.snu.ac.kr/members/">Young Min Kim</a>
                        <span style="font-size:12px;">(*Equally contributed)</span>
                      </p>
                      <p class="research-venue"><em>European Conference on Computer Vision (ECCV)</em>, 2024</p>
                      <p class="research-links">
                        <a href="https://changwoonchoi.github.io/i2slam/">Project Page</a> / <a href="https://arxiv.org/abs/2407.11347">arXiv</a> / <a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/03857.pdf">Paper</a> / <a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/03857-supp.pdf">Supp</a>
                      </p>
                      <p class="research-abstract">
                        <u>How to make neural SLAM robust in real-world data?</u><br>
                        In this paper, we invert imaging process to improve robustness and accuracy of dense SLAM in real-world data which frequently contains motion blur and varying appearances.
                      </p>
                    </div>
                  </article>

                  <article class="research-card">
                    <div class="research-card-thumbnail">
                      <img src="images/slide.png" alt="SLiDE thumbnail">
                    </div>
                    <div class="research-card-content">
                      <h3 class="research-title">
                        <a href="#">SLiDE: Self-supervised LiDAR De-snowing through Reconstruction Difficulty</a>
                      </h3>
                      <p class="research-authors">
                        <span class="research-author-highlight">Gwangtak Bae</span>,
                        <a href="https://bjkim95.github.io/">Byungjun Kim</a>,
                        <a href="https://scholar.google.com/citations?hl=en&user=SNiChqkAAAAJ">Seongyong Ahn</a>,
                        <a href="https://scholar.google.com/citations?hl=en&user=qhWoNw8AAAAJ">Jihong Min</a>,
                        <a href="https://sites.google.com/site/iwshimcv/home">Inwook Shim</a>
                      </p>
                      <p class="research-venue"><em>European Conference on Computer Vision (ECCV)</em>, 2022</p>
                      <p class="research-links">
                        <a href="https://arxiv.org/abs/2208.04043">arXiv</a> / <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136990277.pdf">Paper</a> / <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136990277-supp.pdf">Supp</a>
                      </p>
                      <p class="research-abstract">
                        <u>How to make LiDAR robust in adverse weather?</u><br>
                        We propose a self-supervised LiDAR de-noising method that removes noise points in snowy weather, which is one of the biggest challenges for 3D perception in autonomous driving.
                      </p>
                    </div>
                  </article>
                </div>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Source code is modified from <a href="https://jonbarron.info/">Jon Barron's website</a>.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
    <script>
      (function() {
        var toggle = document.getElementById('news-toggle');
        if (!toggle) return;
        var moreItems = document.querySelectorAll('.news-more');
        toggle.addEventListener('click', function(e) {
          e.preventDefault();
          var expanded = toggle.getAttribute('aria-expanded') === 'true';
          moreItems.forEach(function(li) { li.style.display = expanded ? 'none' : 'list-item'; });
          toggle.setAttribute('aria-expanded', !expanded);
          toggle.textContent = expanded ? 'Show more ▼' : 'Show less ▲';
        });
      })();
    </script>
  </body>
</html>
